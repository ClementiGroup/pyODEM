{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyFExD Proteins\n",
    "\n",
    "This Notebook will help guide you through fitting the epsilon parameters of a protein structure based model using the new pyfexd package. Unlike previous packages, we no longer attempt to automate everything. Instead, we automate the parts that do the \"heavy lifting\" (computing optimal epsilons) while allowing the automation for the more lighter things (file selection) to be handeled by the end user. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's get the basic imports done. Then we'll define a few keyword packages for easy access of the key methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generic Packages\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import mdtraj as md\n",
    "import shutil\n",
    "import ConfigParser\n",
    "import pyemma.coordinates as coor\n",
    "\n",
    "#In house packages\n",
    "import model_builder as mdb\n",
    "import pyfexd\n",
    "\n",
    "#Shortcuts\n",
    "ml = pyfexd.model_loaders #sub-package with methods for loading using model_builder and formatting for pyfexd\n",
    "observables = pyfexd.observables #sub-package for formatting observable information\n",
    "ene = pyfexd.estimators.max_likelihood_estimate #method for running a max-likelihood estimate for new epsilons\n",
    "\n",
    "def save_ini_file(inifile, newiter, newfile_location):\n",
    "    config = ConfigParser.SafeConfigParser(allow_no_value=True)\n",
    "    config.read(inifile)\n",
    "\n",
    "    config.set(\"fitting\", \"iteration\", str(newiter))\n",
    "    newparams = \"%s/model_params\" % newfile_location\n",
    "    newpairwise = \"%s/pairwise_params\" % newfile_location\n",
    "\n",
    "    config.set(\"model\", \"pairwise_params_file\", newpairwise)\n",
    "    config.set(\"model\", \"model_params_file\", newparams)\n",
    "\n",
    "\n",
    "\n",
    "    shutil.move(inifile,\"1.%s\" %inifile)\n",
    "\n",
    "    with open(inifile,\"w\") as cfgfile:\n",
    "        config.write(cfgfile)\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the packages and names are loaded and defined, lets start making objects to prepare for optimization. The first step is to make a `Protein` object, which is a subclass of `Model`. A `Model` object is needed primarily for:\n",
    "\n",
    "1. Loading data from input files (traj.xtc).\n",
    "2. Generating Hamiltonian(epsilon) functions.\n",
    "\n",
    "Each `Model` subclass is customized for a particular use case. In this case, `Protein` refers to a case where we use the `model_builder` package and the Hamiltonian is linear in the epsilons. The specification of temperature is required for properly calculating the scale kT for the maximum liklihood procedure. As an aside, you can also set beta and the temperature will be set automatically.\n",
    "\n",
    "The object `pmodel`  (`Protein` object) will be passed to the solver later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model according to ww_domain.ini\n",
      "Options not shown default to None\n",
      "Model options:\n",
      "  topology             = cacb.gro\n",
      "  bead_repr            = CACB\n",
      "  pairwise_params_file = pairwise_params_repulsive\n",
      "  model_params_file    = model_params\n",
      "  cb_volume            = flavored\n",
      "\n",
      "Fitting options:\n",
      "  iteration            = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jchen/code/model_builder/models/potentials/util.py:7: UserWarning: Using default SBM parameters\n",
      "  warnings.warn(\"Using default SBM parameters\")\n",
      "/home/jchen/code/model_builder/models/potentials/util.py:10: UserWarning: Using default SBM parameters\n",
      "  warnings.warn(\"Using default SBM parameters\")\n"
     ]
    }
   ],
   "source": [
    "temperature = 119\n",
    "\n",
    "model_name = \"ww_domain.ini\"\n",
    "pmodel = ml.Protein(model_name)\n",
    "pmodel.set_temperature(temperature) # for scaling kT\n",
    "iteration = pmodel.fittingopts[\"iteration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's done, let's generate an `ExperimentalObservables` object. This object is meant to hold all the observed experimental data. For our particular example, we want to load a histogram data file. To do so, we will create the object and then add the experimental data using `add_histogram()`. In the future, different data types will be implemented as `add_X`. Some useful hints:\n",
    "\n",
    "1. The first argument is always the experimetal data .dat file.\n",
    "2. `errortype` specifies the probability distribution. WARNING: Not implemented for anything other than gaussians yet.\n",
    "3. `scale` specifies how much to scale the standard deviation by. This does not affect the final result other than scaling the final value of Q that is calculated. The Optimal minima will not change with scaling. \n",
    "\n",
    "The format of the experimental data in the .dat file is simple. The first column gives the value of some observable and the second column gives its standard deviation. Right now, all observables are assumed to follow a gaussian random distribution, but different distributions can be added in the code in the future. Furthermore, you must also give each `add_X` command enough information to construct the observation from a simulation. For a histogram, this can be the edges of each bin, or the range and number of bins, or a spacing and range. For our example, we use the edges of each bin. I have found this to be the most robust for reproducing the same observable as the edges used in histogramming a data set are directly outputted form numpy's histogram command.\n",
    "\n",
    "The scaling is performed by applying the multiplying the given factor to the standard deviation of the observables. It exists as a numerical trick to prevent the derivatives from becoming too large and for Q to not run into `UnderflowError`. Furthermore, you will notice that the factor for scaling is different for the Q-data and the Distance-data. This is because it was found empirically that a smaller scale factor on the Q-data enforced the restraints better, and a larger factor (equal footing to the Distance-data per-say) was not strong enough to prevent the model from Protein from becoming too collapsed.\n",
    "\n",
    "The `ExperimentalObservables` will be passed to the solver later. It contains the means of computing the observables from a simulation, automatically formatted internally, as well as methods for computing the Q value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obs = observables.ExperimentalObservables() #initialize object\n",
    "\n",
    "#define the files for the q distribution formed in the AA model\n",
    "qexp_file = \"%s/q_analysis/exp_data_shaw.dat\" % cwd\n",
    "qedges_file = \"%s/q_analysis/edges_qdata.dat\" % cwd\n",
    "\n",
    "#define information corresponding to the particular distance fitting that matters:\n",
    "file_pair = [79,211]\n",
    "fit_pair = [9, 24]\n",
    "dist_exp_file = \"%s/distance/exp-data_%d-%d.dat\" % (cwd,file_pair[0],file_pair[1])\n",
    "dist_edges_file = \"%s/distance/edges_%d-%d.dat\" % (cwd,file_pair[0],file_pair[1])\n",
    "\n",
    "# Now passs the files to the obs object and it will handle the rest. Note, the order is important\n",
    "obs.add_histogram(qexp_file, edges=np.loadtxt(qedges_file), errortype=\"gaussian\", scale=200.0) \n",
    "obs.add_histogram(dist_exp_file, edges=np.loadtxt(dist_edges_file), errortype=\"gaussian\", scale=1000.0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With everything loaded, the next thing to do would be to load the xtc file. Like I said, we have to specify the location of these files for loading, but the loaded values will be formatted for later use automatically. All `Model` objects have a `load_data()` command that can extract the data set from some file format. In this case, the `Protein` object uses `mdtraj` and can read any format that `mdtraj` can read. \n",
    "\n",
    "Furthermore, we need to extract the data for the observable from the simulation. This is not implemented to happen automatically as specific ways for getting the data varies strongly between different data types, but for a histogram it's simple. We just want the distance between two atoms on the FRET coordinate. This too will be passed as a list, where each entry corresponds to data for each type of observable loaded. For our case, the q_data was computed automatically using the `q_analyze.py` script while the distance data is computed on the fly.\n",
    "\n",
    "The loaded data (`data`) as well as the experimental data (`exp_data`) will be passed to the solver later as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model data\n",
    "directory = \"%s/iteration_%d/%d\" % (cwd,iteration,temperature)\n",
    "os.chdir(directory)\n",
    "data = pmodel.load_data(\"traj.xtc\")\n",
    "\n",
    "#experimental data\n",
    "#q previously computed:\n",
    "qdata = np.loadtxt(\"%s/q_analysis/I%d_T%d_qvalue.dat\" % (cwd, iteration, temperature))\n",
    "#the actual distance\n",
    "traj = md.load(\"traj.xtc\", top=\"conf.gro\")\n",
    "dist = md.compute_distances(traj, [fit_pair], periodic=False)[:,0]\n",
    "obs_data = [qdata, dist]\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we need to discretize the data into bins as well as compute the initial equilibrium distribution. This too is yet automated as there are many unique ways of doing so. Just to name a few:\n",
    "\n",
    "1. You can simply pick a coordinate and discretize along there (such as the FRET distance on a histogram, or number of native contacts formed)\n",
    "2. You can compute TICA and a Markove State Model using the pyEMMA package. \n",
    "3. You can use TRAM (pyEMMA) and combine many data sets.\n",
    "\n",
    "Right now, option 1 was found to be sufficient, and sometimes superior. Lets do a discretization along both the Q and Distance observables into uniformaly spaced bins. Luckily, `PyEMMA` has a clustering method that already works well for this case. With a good cordinate, this should work well. The `equilibrium_frames` is a list where each entry is a list of indices corresponding to the frames inside that equilibrium state. The solver will then automatically compute the stationary distribution from this data set. It can be overrided passing the `stationary_distributions` kwarg to the solver.\n",
    "\n",
    "The `equilibrium_frames` will be passed to the solver later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting output of RegularSpaceClustering: 100% (3/3) [#############] eta 00:01 \\Number of equilibrium states are : 278\n"
     ]
    }
   ],
   "source": [
    "all_dist = np.array(obs_data).transpose()\n",
    "reg_space_obj = coor.cluster_regspace(all_dist, dmin=0.05)\n",
    "dtrajs = np.array(reg_space_obj.dtrajs)[0,:]\n",
    "assert np.min(dtrajs) == 0\n",
    "assert np.shape(dtrajs)[0] == np.shape(data)[0]\n",
    "print \"Number of equilibrium states are : %d\" % (np.max(dtrajs))\n",
    "equilibrium_frames = []\n",
    "indices = np.arange(np.shape(data)[0])\n",
    "for i in range(np.max(dtrajs)+1):\n",
    "    state_data = indices[dtrajs == i]\n",
    "    if not state_data.size == 0:\n",
    "        equilibrium_frames.append(state_data)\n",
    "\n",
    "# Confirm nothing was accidentally missed\n",
    "total_check = 0\n",
    "for set_of_frames in equilibrium_frames:\n",
    "    total_check += len(set_of_frames)\n",
    "assert total_check == np.shape(data)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a required input, but a very good idea. One of the things that can go wrong in an optimization step is if the values of the epsilons reach unphysical values. For example, there's no way an epsilons should have a value of 5. In order to prevent this, we can pass an ordered list, where each entry gives a list with a lower bound and upper bound for  that epsilon. For us, let's keep a pretty tight restriction to avoid overfitting on the initial step. Let's make it so the epsilons can't change by +/-0.3 and no epsilon value may exceed 2 or go under -2. Furthermore, let's make the switch into repulsive contacts dampened where it must first become repulsive before that epsilon is allowed to go under -0.1 in that iteration step. This again was an emirical observation. Allowing a contact to become repulsive in a large step meant that many contacts would never be visited again to know it should be deactivated. \n",
    "\n",
    "Furthermore, you can specify any other feature `numpy.optimization` package can take. I also like to specify `gtol` which is the gradient value before it terminates. For now, I chose a very large value of 0.1, but in fitting I found a value of 0.001 was sufficient. Any lower and fitting procedure has a tendency to go on for an excessive number of cycles. \n",
    "\n",
    "These extra things go into the `function_args` variable as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bounds = []\n",
    "for i,pairwise_pair in enumerate(pmodel.model.Hamiltonian._pairs):\n",
    "    highest = 2\n",
    "    if pairwise_pair.prefix_label == \"LJ12GAUSSIAN\":\n",
    "        lowest = 0\n",
    "    else:\n",
    "        if pairwise_pair.eps >= -0.09:\n",
    "            lowest = -0.1\n",
    "        else:\n",
    "            lowest = -highest\n",
    "    lower_bound = pmodel.epsilons[i] - 0.3\n",
    "    if lower_bound < lowest:\n",
    "        lower_bound = lowest\n",
    "    upper_bound = pmodel.epsilons[i] + 0.3\n",
    "    if upper_bound > highest:\n",
    "        upper_bound = highest\n",
    "    bounds.append([lower_bound,upper_bound])\n",
    "\n",
    "function_args = {\"bounds\":bounds, \"gtol\":0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, all the preliminaries are set up. We can run the whole thing now, and output the new Q value to see how much things have improved. When running the solver, you get a `estimators_class` object as an output, here it's called `solutions`. Many values exist internally such as the new and old epsilons, the new and old Q values and some useful metrics such as how many times functions were called and how long each step took. \n",
    "\n",
    "Furthermore, you can save more information by doing `solutions.save_debug_files()`. This gives the number of times each function is called in case you wanted to optimize your computation. For example, in our case Q was computed 45 times and there were 279 states. The Hamiltonian was computed many more times than Q, as each call for each individual state is computed. Notice the value is equal to 45*279. Nominally, the derivative was computed 37 times indicating the BFGS algorithm only took 37 steps. This is because Q is computed in several output steps and checks, so it is fully expected the number of times Q is computed is greater than the numer of steps the specific optimizer ran for.\n",
    "\n",
    "A note on solvers. Many algorithms for optmization have been implemented. Most of them are wrappers for scipy functions. Currently, the most stable one for proteins that has been extensively used and tested is bfgs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing EstimatorsObject\n",
      "Initializaiton Completed: 0.365583 Minutes\n",
      "Starting Optimization\n",
      "Optimization Complete: 3.503468 minutes\n",
      "\n",
      "Q Functions:\n",
      "Qold: -6.42096e-11\n",
      "Qnew: -0.0531479\n",
      "\n",
      "Log Q Functions:\n",
      "Qold: 23.4689\n",
      "Qnew: 2.93468\n"
     ]
    }
   ],
   "source": [
    "solutions = ene(data, equilibrium_frames, obs, pmodel, obs_data=obs_data, solver=\"bfgs\", logq=True, kwargs=function_args)\n",
    "\n",
    "new_eps = solutions.new_epsilons\n",
    "old_eps = solutions.old_epsilons\n",
    "Qold = solutions.oldQ\n",
    "Qnew = solutions.newQ\n",
    "Qfunction = solutions.log_Qfunction_epsilon\n",
    "\n",
    "print\n",
    "print \"Q Functions:\"\n",
    "print \"Qold: %g\" %Qold\n",
    "print \"Qnew: %g\" %Qnew\n",
    "\n",
    "print\n",
    "print \"Log Q Functions:\"\n",
    "print \"Qold: %g\" %Qfunction(old_eps)\n",
    "print \"Qnew: %g\" %Qfunction(new_eps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all of this done, now, we just need to save all the files and modify the .ini file so that it knows where the new information is. How you do this exactly is up to you. But some code to help you along is shown below. The first block will make a new directory then save a copy of model_params and pairwise_params in that directory as well as an info file containing information on this fitting procedure. The second block will modify the appropriate parameters in the .ini file according to this save scheme. Simply uncomment the `save_ini_file()` command when ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_dir = \"%s/newton_%d\" % (cwd, iteration) \n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "    \n",
    "pmodel.save_model_parameters(new_eps) # load the new epsilons in the model_builder object\n",
    "os.chdir(save_dir)\n",
    "writer = mdb.models.output.InternalFiles(pmodel.model)\n",
    "writer.write_pairwise_parameters()\n",
    "f = open(\"info.txt\", \"w\")\n",
    "f.write(\"computed at temperature: %d\\n\" % temperature)\n",
    "f.write(\"Qold: %g\\n\" % Qfunction(old_eps))\n",
    "f.write(\"Qnew: %g\\n\" % Qfunction(new_eps))\n",
    "f.close()\n",
    "#solutions.save_debug_files() #optional if you wanted to get some statistics on number of certain function calls.\n",
    "os.chdir(cwd)\n",
    "#save_ini_file(model_name, iteration+1, \"newton_%d\" % iteration) #write a new ini file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
