{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyODEM Framework - Introduction\n",
    "\n",
    "This Notebook will help guide you through fitting the epsilon parameters of a protein structure based model using the new pyODEM package. The package is designed so that the \"heavy lifting\" (e.g. maximum likelihood estimation) is automated, while the loading of data files is done by the user through the creation of several objects.\n",
    "\n",
    "The over-arching goal is to take adjust the parameters (epsilon) of a CG-model's Hamiltonian so that the observables the CG model produces matches the observables from some Reference model. This is done iteratively in cycles of \"Simulate -> Optimize -> Simulate -> Optimize...\" until a reasonable agreement between the CG-model observables with the Reference-model observables. Hopefully, the Reference model will simply be real experimental data in the future, but for now we are using an all-atom simulation as a Reference model in order to properly interpret the resultant distribution. \n",
    "\n",
    "As an example, we are going to take the FiP35 ww-domain protein as an example test case. The Reference observables were previously generated from an all-atom model of FiP35, simulated by the D.E. Shaw group. The results of the analysis is included in the folders entitled \"distance\", \"enhance\" and \"q_analysis\". The tutorial will be divided into Three steps. First, a simulation of the coarse-grained model is simulated using the structure-based modeling package from the Onuchic group. Second, some preparation steps are run mainly to prepare observables for analysis while also partitioning the trajectory into discrete clusters of states. In particular, understanding how the partitioning of the trajectory into discrete states occurs would benefit greatly from reviewing the tutorials on the pyEMMA package available form the Noe group in Berlin <http://emma-project.org/latest/ipython.html>. And finally, the last section will deal with setting up a and using the pyODEM to find a new set of parameters for the \"optimize\" step, and then save those new parameters to re-start the \"simulate\" step. All the work in these three sections is to eventually set up the maximum_likelihood object which will be near the end of the final section, expressed as:\n",
    "\n",
    "```\n",
    "solutions = pyODEM.estimators.max_likelihood_estimate(data_formatted, dtrajs, obs, \n",
    "    pmodel, stationary_distributions=stationary_distribution, obs_data=obs_data, \n",
    "    solver=\"bfgs\", logq=True, kwargs=function_args)\n",
    "```\n",
    "\n",
    "Furthermore, if you do not wish for simulations to complete to just test the package, you can copy the directory \"example_2/pre_determined_results/iteration_0\" to \"example_2/iteration_0\" and skip the step in Part I that involves making a shell call to GROMACS commands. \n",
    "\n",
    "\n",
    "## Prerequisites:\n",
    "Before starting, please ensure the following pre-requisites are met:\n",
    "\n",
    "#### Install\n",
    "* pyEMMA : http://emma-project.org/latest/INSTALL.html\n",
    "\n",
    "* generic python packages : numpy, scipy, os, mdtraj, shutil, ConfigParser, subprocess \n",
    "\n",
    "#### Clone\n",
    "The following packages should be cloned into a directory called \"code\" and ensure \".../code\" is in your PYTHONPATH variable:\n",
    "\n",
    "* pyODEM : https://github.com/ClementiGroup/pyODEM\n",
    "\n",
    "* model_builder : https://github.com/ajkluber/model_builder\n",
    "\n",
    "* simulation : https://github.com/ajkluber/simulation\n",
    "\n",
    "## Summary of Key Variables and Terms\n",
    "Some key objects and methods are defined here for easy reference. \n",
    "\n",
    "### Vocbulary\n",
    "* **AA model**: All-Atom model.\n",
    "* **CG model**: Stands for Coarse-Grained model. A common abbreviation used throughout.\n",
    "* **epsilon(s)**: The model parameter(s) that are being adjusted during the pyODEM framework. \n",
    "* **topology**: Can refer to two things. For `model_builder` it refers to the reference structure used to determine parameterize the backbone terms in the Hamiltonian. For GROMACS, it refers specifically to the .top file used for running simulations.\n",
    "\n",
    "\n",
    "### Variable Names in Code:\n",
    "#### Abbreviation on sizes:\n",
    "* N: Number of frames in the trajectory\n",
    " \n",
    "#### Final Objects Created\n",
    "* `data_formatted`:\n",
    "* `dtrajs`: \n",
    "* `obs`:\n",
    "* `pmodel`:\n",
    "* `stationary_distribution`:\n",
    "* `obs_data`:\n",
    "* `function_args`:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once pre-requisites are installed/cloned and ready, the following code will import all the necessary packages. Any of the other missing packages can normally be installed using conda via `conda package`. Then we'll define a few shortcuts for easy access to methods later (`ml`, `observables`, `ene`), as well as `cwd` (current directory) for navigating the directory space inside python. As well as a method for saving ini files, the parameter file for the model_builder package interprets. These imports will be used throughout the tutorial, however not all packages will be used in all sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Generic Packages\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import mdtraj as md\n",
    "import shutil\n",
    "import configparser\n",
    "import pyemma.coordinates as coor\n",
    "import pyemma.msm as msm\n",
    "import subprocess as sb\n",
    "import simulation.gromacs.mdp as mdp_writer\n",
    "\n",
    "#In house packages\n",
    "import model_builder as mdb\n",
    "import pyODEM\n",
    "\n",
    "# initialize MPI, only necessary for Optimization step. \n",
    "# mpi4py does not initialize by default, disable these 2 lines or enable as needed.\n",
    "from mpi4py import MPI\n",
    "MPI.Init()\n",
    "\n",
    "#Shortcuts\n",
    "ml = pyODEM.model_loaders #sub-package with methods for loading using model_builder and formatting for pyODEM\n",
    "observables = pyODEM.observables #sub-package for formatting observable information\n",
    "ene = pyODEM.estimators.max_likelihood_estimate #method for running a max-likelihood estimate for new epsilons\n",
    "\n",
    "def save_ini_file(inifile, newiter, newfile_location):\n",
    "    config = ConfigParser.SafeConfigParser(allow_no_value=True)\n",
    "    config.read(inifile)\n",
    "\n",
    "    config.set(\"fitting\", \"iteration\", str(newiter))\n",
    "    newparams = \"%s/model_params\" % newfile_location\n",
    "    newpairwise = \"%s/pairwise_params\" % newfile_location\n",
    "\n",
    "    config.set(\"model\", \"pairwise_params_file\", newpairwise)\n",
    "    config.set(\"model\", \"model_params_file\", newparams)\n",
    "\n",
    "\n",
    "\n",
    "    shutil.move(inifile,\"1.%s\" %inifile)\n",
    "\n",
    "    with open(inifile,\"w\") as cfgfile:\n",
    "        config.write(cfgfile)\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Running Simulations (~1 hour)\n",
    "This section will detail how to use the `model_builder` package to prepare and run a CG model. All CG models breakdown the Hamiltonian to backbone terms (bonds, angles and dihedral) and parwise terms. The backbone terms determine the local interaction between chemically bonded atoms in the protein, while the pairwise terms represent interactions (e.g. hydrophobicity, electrostatics, etc.) between parts of the protein. You can consult \"C. Clementi, H. Nymeyer, J. Onuchic. *Journal of Molecular Biology* (2000) **298**, 937-953\" for an example of what the Hamiltonian would look like. \n",
    "\n",
    "The `model_builder` package is set up so that any external files being used are indexed from 1 while all internal lists are indexed from 0. For example, in the `cacb.gro` file, 1Gly is used instead of 0Gly, while inside the model_builder package, it is atom 0. \n",
    "To construct a CG model, In particular, for our example of FiP35, the relevant files are:\n",
    "\n",
    "1. `ww_domain.ini`:\n",
    "This is a configuration file for listing the relevant parameters and files for constructing a CG model using the `model_builder` package. In particular, there are two headings [model] and [fittingopts] that are read. The breakdown of the options in each heading is as follows:\n",
    "\n",
    "  **1.1.** [model] section gives a list of options for representing the energy terms of the CG model\n",
    "    * *topology*: A structure file (e.g. pdb, gro, etc.) that must be loadable with `mdtraj`. This file will be used to determine the backbone parameter files like the bond, angle and dihedral. For our FiP35 example, the file cacb.gro is used to determine backbone parameters. An all-atom pdb can be given, where `model_builder` will then parse the file and automatically construct a coarse representation internally. Not to be confused with the GROMACS usage of \"topology\" to refer to the .top file used for running simulations.\n",
    "    * *bead_repr*: The type and level of coarse-graining, i.e. CA (C_alpha), CACB (C_alpha-C_beta), and awsem (AWSEM three-bead model). For FiP35, it is set to CACB to use a C_alpha-C_beta level of coarse-graining (one backbone bead and one side-chain bead).\n",
    "    * *pairs*: A file path, where the file contains a two-column list of CG atoms forming pairwise contacts, indexed from     1. A Leonnard-Jones 12-10 potential is added for each atom on this list, with the minima of the potential corresponds to the distance between the atoms in the topology given above. This is not in the ww_domain.ini file as instead the pairwise_params_file option is used to allow for a greater degree of control over the atom parameters. \n",
    "    * *pairwise_params_file*: A file path, where each line of the file denotes a pairwise interaction including the pairs, type of function and function parameters. See below for a full explanation. This essentially allows full control of the pairwise prameters in the Hamiltonian. \n",
    "    * *model_params_file*: A file path, where each line gives the epsilon values for the pairwise parameters. The number of lines in this file is the same as the number of files in the pairwise_params_file, and gives the epsilon for its corresponding pairwise potential. These are the parameters used in the optimization framework. \n",
    "    * *cb_volume*: Used specifically when bead_repr=CACB, with possible options including cb_volume, average or flavored. Setting cb_volume=flavored means the size of the CB atoms are selected based upon the type of residue, where larger side-chains have larger volumes. \n",
    "\n",
    "  **1.2.** [fittingopts] section gives a list of options for keeing track of parameters used during the optimization process. None of these parameters have any bearing on writing out the files for running simulations. \n",
    "    * *iteration*: This is used to keep track of which iteration of the pyODEM framework you are on. For example, it determines which files and directories to write.\n",
    "\n",
    "2. `cacb.gro`:\n",
    "Structure file that the backbone parameters of the Hamiltonian are based off of (ideal bond distance, ideal angle, ideal dihedral angle). This .gro file has already been constructed by placing the CA atom at the location of the alpha-Carbon and the CB atom at the center-of-mass of the side-chain from an all-atom FiP35 representation. An all-atom structure file also would have been acceptable and would have been processed by model_builder internally to conform to the desired level of coarse-graining. \n",
    "\n",
    "3. `pairwise_params_repulsive`: \n",
    "This is the pairwise params file used in the .ini option \"pairwise_params_file\". The file consists of comment lines with the hashtag symbol (#), followed by columns of information, with each line corresponding to a specific pairwise interaction. The column widths do not need to be an exact width but require white-space between columns. For each line, a pairwise interaction between atoms i and j in the CG representation:\n",
    "\n",
    "  **3.1.** Breakdown of Columns:\n",
    "    * Column 1: Atom i, indexed from 1 in the CG representation.\n",
    "    * Column 2: Atom j, indexed from 1 in the CG representation.\n",
    "    * Column 3: A count of the number of pairwise function's index (indexing from 0 as it is used internally). \n",
    "    * Column 4: A string giving the type of potential (e.g. LJ1210, LJ126, LJ12GAUSSIAN)\n",
    "    * Column 5-: All columns starting with the fifth column are used to parameterize the pairwise potential.\n",
    "    \n",
    "  **3.2.** Example for FiP35:\n",
    "    * Column 1 - Column 2: The interactions are only defined between CA-CA pairs and CB-CB pairs that are not connected by any backbone interactions (bond, angle, or dihedral), i.e. CA-CA pairwise interactions are atleast 4 residues apart.\n",
    "    * Column 3: Counts from 0 to 932, and corresond to the list-index of that potential in the Hamiltonian._pairs list, see Appendix I below for more details on accessing potentials inside the Hamiltonian.\n",
    "    * Column 4: This is either LJ12GAUSSIAN or LJ12GAUSSIANTANH. Please refer to  \"R.R. Cheng, M. Raghunathan, J.K. Noel and J.N. Onuchic *Protein Science* (2016) **25**, p114\" where equations 5 and 6 give the functional form of the LJ12GAUSSIAN and LJ12GAUSSIANTANH respectively. Also, refer to the website: http://smog-server.org/extension/gauss.html ,for a good explanation of the advantage of the LJ12GAUSSIAN potential over the Leonnard-Jones-12-10 potential (LJ1210) traditionally used in CG models. The purpose of these LJ12GAUSSIAN type potentials is to provide a attractive and extra repulsive potential while maintaining the same hard-wall 1/r^12 excluded volume interaction regardless of the strength of the attractive/repulsive well. LJ12GAUSSIAN is used for pairs forming contacts in the native state, while LJ12GAUSSIANTANH is used for all pairs not forming native contacts in the native state. LJ12GAUSSIANTANH also allows the contact to not only have a Gaussian attractive well, but a slightly more repulsive Tanh function barrier. \n",
    "    * Column 5: Gives the excluded valume r_0 used in the (r_0/r)^12 part of both functions.\n",
    "    * Column 6: Gives the minima of the Gaussian well, or x0 when tanh(x-x0) = 1/2. \n",
    "    * Column 7: Gives the standard-width of the Gaussian well, and the shift for the Tanh function. \n",
    "\n",
    "4. `model_params`:\n",
    "The `model_params` file contains all the parameters, `epsilon`, that will be changed during the pyODEM optimization step. Each line corresponds to the pairwise potential of the same line in pairwise_params_repulsive. The `epsilon` gives the scale of the attractive or extra repulsive barrier. For example, in the LJ12GAUSSIAN, the value of `epsilon` is constrained to be greater than 0 and larger values denote deeper (stronger) attractive wells (forces). For the LJ12GAUSSIANTANH potentials, the value of `epsilon` can switch between positive and negative values. A negative `epsilon` means the repulsive LJ12TANH function is used, with the absolute value of the `epsilon` scaling the height of the repulsive barrier. A positive value then means the LJ12GAUSSIAN attractive well potentail will be used. \n",
    "\n",
    "5. Determine \"native\" contacts: There are several ways of determining native contacts, such as just taking atoms (e.g. sharp cutoff). In our case, we use the SMOG shadow contact map to determine our native contacts <http://smog-server.org/Shadow.html>. This was previously done and used in generating the original `pairwise_params_repulsive` and `model_params` files. \n",
    "\n",
    "This section will mainly use the `model_builder` package, which was abbreviated as `mdb` during the import step for writing out topology files for GROMACS, and the `simulation` package for writing out GROMACS .mdp files (`mdp_writer`). Furthermore, the `os` and `subprocess` (`sb`) package is used for making directories and executing GROMACS commands. Running a simulation will approximately an hour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model according to ww_domain.ini\n",
      "Options not shown default to None\n",
      "Model options:\n",
      "  topology             = cacb.gro\n",
      "  bead_repr            = CACB\n",
      "  pairwise_params_file = pairwise_params_repulsive\n",
      "  model_params_file    = model_params\n",
      "  cb_volume            = flavored\n",
      "\n",
      "Fitting options:\n",
      "  iteration            = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jchen/code/model_builder/models/potentials/util.py:7: UserWarning: Using default SBM parameters\n",
      "  warnings.warn(\"Using default SBM parameters\")\n",
      "/home/jchen/code/model_builder/models/potentials/util.py:10: UserWarning: Using default SBM parameters\n",
      "  warnings.warn(\"Using default SBM parameters\")\n"
     ]
    }
   ],
   "source": [
    "os.chdir(cwd) # define current working directory\n",
    "\n",
    "# Set the temperature of the simulation\n",
    "# T=120 is near the folding temperature for most CG models. \n",
    "temperature = 119 \n",
    "\n",
    "# Read input files. The model_builder package is used to read the ww_domain.ini:\n",
    "    # model is an object containing all the Hamiltonian parameters\n",
    "    # fitopts is a dictionary containing the fitting options, in this case, \"iteration\"\n",
    "model, fitopts = mdb.inputs.load_model(\"ww_domain.ini\") # model_builder reads the ww_domain.ini file.\n",
    "\n",
    "# Current iteration. Describes which directories to write to. \n",
    "iteration = fitopts[\"iteration\"] # current iteration\n",
    "\n",
    "# starting configuration for the MD simulation. \n",
    "    # Any structure at the correct level of coarse-graining can be used\n",
    "    # If not used, default is to use the model_builder topology as the starting structure\n",
    "starting_conf = \"%s/cacb.gro\" % (cwd)  \n",
    "model.set_starting_conf(md.load(starting_conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directories to put gromacs files\n",
    "iter_dir = \"%s/iteration_%d\" % (cwd, iteration) #one directory for each iteration\n",
    "temp_dir = \"%s/%d\" % (iter_dir, temperature) #a sub-directory for each temperature in cases for multi-temperature runs.\n",
    "\n",
    "if not os.path.isdir(iter_dir):\n",
    "    os.mkdir(iter_dir)\n",
    "if not os.path.isdir(temp_dir):\n",
    "    os.mkdir(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the code below: \n",
    "    # write out files for running a GROMACS simulation into the temperature directory.\n",
    "    # i.e. molecular coordinate force field files (.gro, .top)\n",
    "    # Control files for simulation parametesr (.mdp)\n",
    "    # Refer to GROMACS websites for a full explanation.\n",
    "os.chdir(temp_dir) # change to the temperature directory\n",
    "\n",
    "# a \"writer\" object for outputting gromacs files\n",
    "    # Base the output files from the model paramterized in the \"model\" object\n",
    "writer = mdb.models.output.GromacsFiles(model) \n",
    "writer.write_simulation_files() \n",
    "\n",
    "# A separate package, mdp_writer = simulation.mdp, for writing .mdp files\n",
    "    # The .mdp has all the options for controlling the the GROMACS simulation\n",
    "    # Temperature = 119, near the folding temperature.\n",
    "    # Use a time step of 0.0005ps, and save every 1000 frames.\n",
    "    # 3000000 time steps is a 15ns total run with N=30,000 frames saved.\n",
    "mdp_string = mdp_writer.constant_temperature(str(temperature), \"30000000\")\n",
    "with open(\"run.mdp\", \"w\") as fout:\n",
    "    fout.write(mdp_string)\n",
    "    \n",
    "os.chdir(cwd) # change back to original directory. Good practice at the end of every code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now actually run the simulation using the subprocess (sb) package. \n",
    "# subprocess makes a call to the shell you are using, and thus can execute commands for MD simulations\n",
    "\n",
    "os.chdir(temp_dir)\n",
    "\n",
    "# pre-step for writing out GROMACS files into a binary format.\n",
    "sb.call(\"grompp_sbm -c conf.gro -f run.mdp -p topol.top -o topol.tpr\", shell=True) \n",
    "\n",
    "# Run the MD simulation\n",
    "sb.call(\"mdrun_sbm -s topol.tpr -nt 1\", shell=True)\n",
    "\n",
    "# change to the home directory when done\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At this point, you should have a file called \"example_2/iteration_0/119/traj.xtc\" which is the trajectory file in binary format. This can be loaded with mdtraj and various GROMACS commands exist for slicing it, extracting energies, etc. \n",
    "\n",
    "Sources of failure:\n",
    "1. grompp_sbm or mdrun_sbm command is not found : The package is not installed correctly. Possibilities include: Installation completely failed. Or the \"bin\" directory needs to be added to \"PATH\" and \"lib\" directory to \"LD_LIBRARY_PATH\". The suffix is different, meaning command might be grompp instead of grompp_sbm depending on your GROMACS installation flags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part II: Pre-Computing Observables + Markov State Models (~5 minutes)\n",
    "This section will deal with pre-computing certain observables that take a while to compute at run time as well as compute a Markov State Model (MSM) after identifying slow coordinates using TICA. However, this section will not detail all the features of the pyEMMA software as Frank Noe's group has developed a very nice set of tutorials detailing their package. The discrete trajectory and the stationary distribution from the MSM will be all you need from pyEMMA to run the optimization.\n",
    "\n",
    "As for the observables, there are three observables we are using for FiP35: The Qc (fraction of native contacts) as well as two Calpha-Calpha distances, hereby nicknamed Qc, Distance and Enhance. The resultant observables will consist of 1-D array of length N, where N is the size of your trajectory. This will later be used during the optimization procedure when comparing observables with the Reference observables for fitting. \n",
    "\n",
    "All of this analysis is on the trajectory you just simulated, the CG trajectory. All the all-atom information has already been pre-computed and will be loaded in the next step. The exact observables you will use for your own systems will likely be different and require some degree of customization, meaning this is a step that will likely be modified heavily in your own work. For help in constructing your own observables, I would refer you to the documentation for the packges used here, as well as the section in Part III for formatting the Reference input files.\n",
    "\n",
    "In particular, we will be using the `mdtraj` package heavily as well as the `model` object from the previous section for computing the observables. Furthermore, the `pyemma` packages `coor` and `msm` (see original import block) will be used heavily for computing a stationary distribution and a discrete trajectory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computing the observables, the trajectory needs to be loaded with `mdtraj`, which creates a `traj` object containing atom coordinates, and other useful functions for working with the trajectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajfile = \"%s/traj_short.xtc\" % temp_dir\n",
    "#trajfile = \"%s/traj.xtc\" % temp_dir\n",
    "topfile = \"%s/conf.gro\" % temp_dir\n",
    "\n",
    "traj = md.load(trajfile, top=topfile) # make the traj object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and Computing Stationary Distribution\n",
    "\n",
    "This has several steps, but the goal is to get a discrete cluster of states followed by the probability, or stationary distribution, of each state as found from the MSM. In a well sampled system (sampled equilibrium distribution), the stationary distribution from the MSM won't be different from the stationary distribution without an MSM (i.e. N_i / N for N_i conformations in discrete state i with a total of N states). However, in larger systems where the equilibrium distribution is not truly found, a MSM can be used to augment and find the true equilibrium distribution. The steps below will follow this outline:\n",
    "1. Compute TICA on the trajectory.\n",
    "2. Cluster on ~sqrt(N) number of states. This was determined by the Noe group to be an ideal number of clusters.\n",
    "3. Compute an MSM.\n",
    "4. End with the discrete trajectory, `dtrajs`, and the stationary distribution, `stationary_distribution`.\n",
    "\n",
    "This section will make heavy use of the pyemma package, so please do consult their documentation for more information. Rather than repeating all their information here, I'll focus on just making the necessary components. In particular for FiP35, we will use 170 discrete states. \n",
    "\n",
    "Recall, the main purpose of this step is to group structurally similar conformations in the trajectory into discrete states so the whole set of states is re-weighted together rather than each state individually. For that, TICA alone would be sufficient. But we also want to recover the true equilibrium distribution, which will be acquired by computing the MSM. In the end, we will have 170 structurally similar FiP35 discrete states, along with the proper equilibrium probability of each of those states. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19-02-19 17:05:10 pyemma.coordinates.data.featurization.featurizer.MDFeaturizer[0] WARNING  The 1D arrays input for add_inverse_distances() have been sorted, and index duplicates have been eliminated.\n",
      "Check the output of describe() to see the actual order of the features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-b159e90d6b81>:22: PyEMMA_DeprecationWarning: Passed deprecated argument \"chunk_size\", please use \"chunksize\"\n",
      "  inp = coor.source(trajfile, feat, chunk_size=10000) # create an input object with pyemma to stream the data\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 170 # ~ sqrt(30000)\n",
    "msmlag_time = 30 # msm lag time in steps\n",
    "ticalag_time = 10 # tica lag time in steps\n",
    "\n",
    "top = traj.top # topology information in mdtraj format\n",
    "ca_idxs = top.select(\"name CA\") # select all the indices for the Calpha atoms\n",
    "\n",
    "feat = coor.featurizer(topfile) # featurizer object\n",
    "feat.add_distances_ca() # use c-alpha distances in TICA\n",
    "feat.add_inverse_distances(ca_idxs) # use inverse c-alpha distances in TICA\n",
    "\n",
    "\n",
    "# add custom dihedral distances, by computing the dihedral angle in a sequence of four Calpha atoms.\n",
    "num_ca = len(ca_idxs) -3\n",
    "ca_dihedrals = np.zeros((num_ca,4))\n",
    "for i in range(num_ca):\n",
    "    for j in range(4):\n",
    "        ca_dihedrals[i,j] = ca_idxs[i+j]\n",
    "ca_dihedrals = ca_dihedrals.astype(int)\n",
    "feat.add_dihedrals(ca_dihedrals)\n",
    "\n",
    "inp = coor.source(trajfile, feat, chunksize=10000) # create an input object with pyemma to stream the data\n",
    "\n",
    "# compute TICA using the commute map option and keep only the first 50 dimensions.\n",
    "# In principle you should use the option var_cutoff = 0.95, and filter out the negative eigenvalues\n",
    "# However, generally only about the first 50 dimensions matter and/or have non-negative eigenvalues from TICA.\n",
    "tica_obj = coor.tica(inp, lag=ticalag_time, kinetic_map=False, commute_map=True, dim=50) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(Label(value='initialize kmeans++ centers'),), layout=Layout(max_width='35%', min…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HBox(children=(Label(value='kmeans iterations'),), layout=Layout(max_width='35%', min_width='35…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Cluster on the ideal coordinates\n",
    "Y = tica_obj.get_output() # the tica coordinates for each conformation\n",
    "\n",
    "# use a k-mean clustering algorithm with pyemma\n",
    "# compute with a stride=10. This will calculate the cluster centers on every 10 frames\n",
    "# Striding speeds up the calculation and because protein data is strongly time-correlated, won't lose any information.\n",
    "cl = coor.cluster_kmeans(Y, stride=10, k=n_clusters, max_iter=1000) \n",
    "dtrajs = cl.dtrajs[0] # get the dtrajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now compute the MSM on the dtrajs and get the stationary_distribution\n",
    "M = msm.estimate_markov_model(dtrajs, msmlag_time)\n",
    "if M.active_state_fraction == 1:\n",
    "    stationary_distribution = M.stationary_distribution\n",
    "else:\n",
    "    print(\"Error: Not all states were used in the stationary distribution\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qc Observables\n",
    "For FiP35, it was found that some sort of Qc restraint is necessary in order to prevent the conformations from becoming too collapsed. Here, we define Qc as the fraction of native contacts, diverging from literature as we will be using Q for Q-value later. The file \"fip35_contacts.dat\" contains a list of native contacts, as determined from the SMOG shadow contact map. In order to actually compute the native contacts for a CaCb model, we will need to extract the ideal distances (location of minimas for each attractive potential) and use that to determine which contacts are in contact or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Q_traj(traj, pairs, r0):\n",
    "    \"\"\" Compute fraction of native contacts along trajectory\n",
    "    \n",
    "    args:\n",
    "    traj (mdtraj.Trajectory): Input trajectory to compute contacts, of length N.\n",
    "    pairs (list of int): len M, native contact pairs  \n",
    "    r0 (list of float): len M, ideal pair-distances for native contacts\n",
    "    \n",
    "    returns:\n",
    "    list: Qc computed, of length N. \n",
    "    \"\"\"\n",
    "    distances = md.compute_distances(traj, pairs, periodic=False)\n",
    "    Q_matrix = np.zeros(np.shape(distances))\n",
    "    for idx in range(np.shape(distances)[0]):\n",
    "        for jdx in range(np.shape(distances)[1]):\n",
    "            if distances[idx,jdx] > (r0[jdx]*1.25):\n",
    "                Q_matrix[idx,jdx] = 0\n",
    "            else:\n",
    "                Q_matrix[idx,jdx] = 1\n",
    "            assert Q_matrix[idx, jdx] >= 0\n",
    "    Q_val = np.sum(Q_matrix, axis=1) / float(len(pairs))\n",
    "    assert np.shape(Q_val)[0] == traj.n_frames\n",
    "    return Q_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_pairs_list = [] # list of native pairs\n",
    "native_r0_list = [] # list of ideal r0 distances\n",
    "\n",
    "# format the native pairs\n",
    "native_array_pairs = np.loadtxt(\"fip35_contacts.dat\")\n",
    "check_pairs_list = []\n",
    "for idx in range(np.shape(native_array_pairs)[0]):\n",
    "    check_pairs_list.append([native_array_pairs[idx,0]-1, native_array_pairs[idx,1]-1])\n",
    "\n",
    "# iterate over each pairiwse potential in the model Hamiltonian and save the ideal distance r0\n",
    "for pot in model.Hamiltonian._pairs:\n",
    "    if [pot.atmi.index,pot.atmj.index] in check_pairs_list or [pot.atmj.index,pot.atmi.index] in check_pairs_list:\n",
    "        r0 = pot.r0\n",
    "        native_pairs_list.append([pot.atmi.index, pot.atmj.index])\n",
    "        native_r0_list.append(r0)\n",
    "\n",
    "observable_qc = compute_Q_traj(traj, native_pairs_list, native_r0_list) # the Qc observable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Distance Observables\n",
    "For FiP35, there are two distances that were determined to be particularly relevant from the all-atom Reference trajectory. In particular, the Calpha-to-Calpha distance to Pro6-Arg14 and Ser2-Arg17 (nicknamed \"distance\" and \"enhance\") were found to be particularly good observables to use. These are between the all-atom pairs 79-211 and 11-254 respectively. For the CG CaCb representation, distance and enhance are between atom pairs 9-24 and 1-29. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pro6-Arg14,  Ser2-Arg17\n",
    "file_pairs = [[79,211], [11,254]]\n",
    "fit_pairs = [[9, 24], [1,29]]\n",
    "suffixes = [\"distance\", \"enhance\"]\n",
    "\n",
    "observable_distance = md.compute_distances(traj, [fit_pairs[0]], periodic=False)[:,0] # compute for 'distance'\n",
    "observable_enhance = md.compute_distances(traj, [fit_pairs[1]], periodic=False)[:,0] # compute for 'enhance'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Running the Optimization Step (~5 minutes ~ 1 Hour)\n",
    "Finally, we arrive at using the pyODEM package. The goal for this example is to adjust the CG model's pairwise parameters, epsilons, until the CG model's observables agree with the target values, which are the observables from the all-atom Reference model. The current use-case involves creating a few objects to hold the data, and then throwing into a function and computing the optimal coordinates. This section will however be set up a little differently. First, the creation of the pyODEM specific objects (`Protein` and `obs`) will be discussed. Then, the loading of the Reference data from the all-atom simulation will be demonstrated and some discussion will cover how to format the Reference data files. Finally, the optimization procedure will be run and options for adjusting the optimization procedure will be discussed. \n",
    "\n",
    "To double-check our purpose, remember we started with an initial set of epsilons (pairwise parameters) from the \"model_params\" file that we want to improve. We simulated FiP35 and got 30,000 independent conformations (frames). From there, we clustered the conformations into 170 discrete states and computed a trajectory between those discrete states, `dtrajs`, as well as a probability for each discrete state, `stationary_distribution`. The observables for each conformation were computed in `observable_qc`, `observable_distance`, and `observable_enhance`, and will be used for the `pyODEM` object, `obs`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to make a `Protein` object called `pmodel`, which is a subclass of `Model`. Inside the optimization procedure, a `Model` object is used for:\n",
    "\n",
    "1. Loading and formatting the data from input files (traj.xtc).\n",
    "2. Generating Hamiltonian(epsilon) functions.\n",
    "\n",
    "Each `Model` subclass is customized for a particular use case. In this case, `Protein` refers to a case where we use the `model_builder` package's Hamiltonian methods, and we assume the Hamiltonian varies lienarly with the epsilons. \n",
    "\n",
    "The main methods of concern are:\n",
    "\n",
    "**`Protein.set_temperature(temperature)`** : Set the temperature for the model, automatically setting the kT scale for the simulation.\n",
    "\n",
    "**`Protein.load_data(trajfilename)`** : Read the specified traj-file and load and format the data for the model to compute the energy later. For FiP35, this would format the output as a NxP array, for N-frames and P pair-wise interactions. Typically, this data_model is handeled inside a helper function.\n",
    "\n",
    "\n",
    "Also, these attributes might be useful:\n",
    "\n",
    "**`pmodel.model`** : the model objection you would also get in section 1 from model_builder\n",
    "\n",
    "**`pmodel.fittingopts`** : the fittingopts dictionary you would also get in section 1 from model_builder. I.e. you can do pmodel.fittingopts[\"iteration\"] to get the iteration.\n",
    "\n",
    "\n",
    "`pmodel`  will be passed to the solver later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model according to ww_domain.ini\n",
      "Options not shown default to None\n",
      "Model options:\n",
      "  topology             = cacb.gro\n",
      "  bead_repr            = CACB\n",
      "  pairwise_params_file = pairwise_params_repulsive\n",
      "  model_params_file    = model_params\n",
      "  cb_volume            = flavored\n",
      "\n",
      "Fitting options:\n",
      "  iteration            = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jchen/code/model_builder/models/potentials/util.py:7: UserWarning: Using default SBM parameters\n",
      "  warnings.warn(\"Using default SBM parameters\")\n",
      "/home/jchen/code/model_builder/models/potentials/util.py:10: UserWarning: Using default SBM parameters\n",
      "  warnings.warn(\"Using default SBM parameters\")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ww_domain.ini\" \n",
    "pmodel = ml.Protein(model_name) # make the Protein object\n",
    "pmodel.set_temperature(temperature) # for scaling kT\n",
    "\n",
    "data_model = pmodel.load_data(trajfile) # load the data from the traj file, this is just an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to make the `ExperimentalObservables` object, which we call `obs` below. This object loads the Reference all-atom observables for FiP35, while also computing the observables for the CG model. For FiP35, we are loading three observables which are then histogrammed into bins. Each are added individually with the `add_histogram()` method. Furthermore, this object computes the quality factor Q. Several methods below will demonstrate how to do so. The quality factor Q consists of the product of the error of the CG model's observables from the Reference all-atom observables. The gaussian is defined as:\n",
    "\n",
    "G_i = exp[(x-x_i)^2 / (2*sigma_i^2)]\n",
    "\n",
    "Where x is the counts in the bin of a histogram for the CG-model  x_i is the counts for a bin for a histogram for the all-atom Reference model, and sigma_i is the standard error for that bin, which is sqrt(x_i). The G_i is computed for each bin of all three histograms for the Qc, Distance and Enhance observables, and then all G_i are multiplied together to get the quality factor Q. Consequently, Q always takes values between 0 to 1, where 1 indicates the CG model agrees with the all-atom Reference model, and 0 indicates that CG model does not agree with the all-atom Reference model. Furthermore, the negative logarithm of Q can be used instead for optimization, where -log(Q) = 0 indicates perfect agreement and -log(Q) > 0 indicates some poor agreement somewhere. This is a much more numerically stable quantity to compute so will be used later.  \n",
    "\n",
    "For the all-atom Reference model, the values for the histogram of all three observables were pre-computed and stored in two files. The \"exp-data\\*.dat\" store the values for each bin of the histogram in the first column, and the standard error sigma_i in the second column. The \"edges\\*.dat\" files store the corresponding edges for the \"exp-data\\*.dat\" files. Consequently, the length of the edges file is one line longer than the exp-data file. The following files are used.\n",
    "\n",
    "distance/exp-data_79-211.dat and distance/edges_79-211.dat : The Distance observable histogram and edges from the all-atom Reference model.\n",
    "\n",
    "enhance/exp-data_11-254.dat and enhance/edges_11-254.dat : The Enhance observable histogram from the all-atom Reference model.\n",
    "\n",
    "q_analysis/exp_data_shaw.dat and q_analysis/edges_qdata.dat : The Enhance observable histogram from the all-atom Reference model.\n",
    "\n",
    "\n",
    "There are several methods for obs detailed here:\n",
    "\n",
    "This method the user will be expected to use:\n",
    "\n",
    "**`ExperimentalObservables.add_histogram()`** : Reads a pre-computed histogram from a file and formats the Gaussian probability function. The one option that matters is \"scale\", which is a constant factor applied to each sigma_i. This is for making a more numerically stable computation, as well as applying emphasis to certain observables. For FiP35, the scale is 200.0 for the Qc observable, but 1000.0 for the Distance and Enhance observables. A smaller scale indicates that the observable should be given a greater importance in the Quality factor than observables with a larger scale. It was found that for FiP35, emphasizing the Qc observable in the Quality factor was required in order to prevent the Protein from becoming too collapsed. \n",
    "\n",
    "These methods the user is not expected to need, and is mostly used internally inside the optimization procedure.\n",
    "\n",
    "**`ExperimentalObservables.compute_observations()`** : Takes a list of length P, consisting of Nx1 arrays, where P is the number of observables and N is the number of conformations. Each array gives the corresponding observed value (i.e. Fraction of native contact, Calpha-Calpha distance) for each frame of the trajectory. Then it histograms the data and outputs the value of the histogram, correctly formatted for use with get_q_functions() below.\n",
    "\n",
    "**`ExperimentalObservables.get_q_functions(self)`** and **`ExperimentalObservables.get_log_q_functions(self)`** : Returns the Q (-log Q) function and a derivate of the Q (-log Q) function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unweighted -log(Q) value is : 34.792880\n"
     ]
    }
   ],
   "source": [
    "obs = observables.ExperimentalObservables() #initialize object\n",
    "\n",
    "# Load and add Qc observable to the obs object with add_histogram()\n",
    "qexp_file = \"%s/q_analysis/exp_data_shaw.dat\" % cwd\n",
    "qedges_file = \"%s/q_analysis/edges_qdata.dat\" % cwd\n",
    "obs.add_histogram(qexp_file, edges=np.loadtxt(qedges_file), errortype=\"gaussian\", scale=200.0) \n",
    "\n",
    "# Load and add the Distance observable to the obs object \n",
    "dist_exp_file = \"%s/distance/exp-data_%d-%d.dat\" % (cwd,file_pairs[0][0],file_pairs[0][1])\n",
    "dist_edges_file = \"%s/distance/edges_%d-%d.dat\" % (cwd,file_pairs[0][0],file_pairs[0][1])\n",
    "obs.add_histogram(dist_exp_file, edges=np.loadtxt(dist_edges_file), errortype=\"gaussian\", scale=1000.0) \n",
    "\n",
    "# Load and add the Enhance observable to the obs object \n",
    "dist_exp_file = \"%s/enhance/exp-data_%d-%d.dat\" % (cwd,file_pairs[1][0],file_pairs[1][1])\n",
    "dist_edges_file = \"%s/enhance/edges_%d-%d.dat\" % (cwd,file_pairs[1][0],file_pairs[1][1])\n",
    "obs.add_histogram(dist_exp_file, edges=np.loadtxt(dist_edges_file), errortype=\"gaussian\", scale=1000.0) \n",
    "\n",
    "obs.prep()\n",
    "\n",
    "# Construct the obs_data list in the same corresponding order as the histograms added\n",
    "obs_data = [observable_qc, observable_distance, observable_enhance]\n",
    "\n",
    "# The following steps are not necessary, but can be used in order to compute current Q value and histograms.\n",
    "# It will print out a number that should be around 30, indicating a very poor optimization. \n",
    "observed_results, observed_standard_deviation = obs.compute_observations(obs_data)\n",
    "log_q_function, d_log_q_function = obs.get_log_q_functions()\n",
    "\n",
    "print(\"unweighted -log(Q) value is : %f\" % log_q_function(observed_results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, now with the implementation of parallelization, it is recommended to use the helper functions to load the data. Implementing the parallelization required the ability to load the protein pair-distances data in parallel so as to not overload the memory requirement for each core. This has the added advantage of allowing us to work with larger data sets, but does require a more complicated input. Instead, we need to have the `pmodel` and `formatted_data` as a list. This list is different for each core, and each entry is a dictionary. For example, if we have C cores and M discrete states, we should have M/C entries for each `formatted_data` on each core.\n",
    "\n",
    "The dictionaries contain:\n",
    "* \"index\" is the discrete state index (i.e. either 0, 1, 2, 3 ,... or 999).\n",
    "* \"data\" is from pmodel.load_data() for the particular sub set of states.\n",
    "* \"obs_result\" is the computed observables from the obs_data. Note, this is not the same as obs_data by any means. This is the result from the `observable_object.compute_observations(...)` function.\n",
    "* \"obs_std\" is the standard deviation for the obs_result, and is computed using the same function as above.\n",
    "\n",
    "This format also allows future options where obs_result isn't easy to cast as a function of obs_data in each frame (i.e.if you want obs_data from a large data set, but you want to re-weight only a small subset of frames). \n",
    "\n",
    "The previous code blocks in this section contains more steps than required for the purposes of pedagogy. Technically you don't need to use a helper function, so long as you can construct a list of dictionaries with the necessary entries on each core. This can be done if you believe you have a better way of dividing up the workload or if the standard methods for computing these values don't apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pmodel, data_formatted = pyODEM.model_loaders.load_protein(dtrajs, trajfile, model_name, observable_object=obs, obs_data=obs_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the pyODEM objects are prepared, it is time to actually optimize the Q value and determine a new set of epsilons. However, first we should determine the parameters for the optimization. The optimization uses a Newton-like search in the parameter space of epsilons to find the epsilons that give the smallest value for -log(Q). Therefore, these kinds of non-linear optimization problems require providing a bounds to restrict the search space as well as a termination criteria for when to stop the search. Two parameters in particular need to be determined by the user. For FiP35, well-determined parameters are already found and are given below, as well as an explanation for why those values were selected.\n",
    "\n",
    "**bounds** : The epsilons are restricted to change only a small amount during the optimization step. A reasonable value to pick would be to bound each epsilon to +/-0.3 of its starting value, which was determined through several attempts of optimization for FiP35. Furthermore, when we compute the bounds, we also ensure that if an epsilon is non-negative, it is restricted to the lowest value of -0.1, to ensure a potential doesn't become repulsive too quickly. This was also determined through trial and error, where a very negative potential severely restrict sampling of that pair. **note:** Allowing large changes such as +/-0.8 would indeed allow the pyODEM framework to change epsilons quickly and search a larger section of parameter space, but such a large step in parameter space is not advised. This is because the quality factor Q depends on the re-weighting of the Hamiltonian by changing the epsilons, that any large step would lead to a poorly defined Hamiltonian. However, decreasing the bounds to say +/-0.1 would allow a smaller step and ensure you are always in a well-defined region for the re-weighted Hamiltonian, but it would also prevent the pyODEM framework from determining the ideal epsilons quickly. The value of +/-0.3 was found to be a reasonable compromise between these two extremes for FiP35. \n",
    "\n",
    "**termination criteria** : The optimization is terminated with gtol. If you want to use a quick optimization for FiP35 just to test, gtol=0.1 allows for a reasonably large change without taking so long. For production runs, gtol=0.001 would be better. Decreasing gtol would require the gradient to become smaller before termination. This can actually lead to meaningful changes in epsilons. Again, gtol=0.001 was found to work well for the FiP35 use case here, but maybe not necessarily for all future use cases. In truth, a rigorous cross-validation method should be used at each iteration step to determine this parameter perfectly, and is in fact currently being implemented.\n",
    "\n",
    "Finally, both of these are added to the the dictionary `function_args` which is passed to the final optimization procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = []\n",
    "for i,pairwise_pair in enumerate(pmodel.model.Hamiltonian._pairs):\n",
    "    highest = 2\n",
    "    if pairwise_pair.prefix_label == \"LJ12GAUSSIAN\":\n",
    "        lowest = 0\n",
    "    else:\n",
    "        if pairwise_pair.eps >= -0.09:\n",
    "            lowest = -0.1\n",
    "        else:\n",
    "            lowest = -highest\n",
    "    lower_bound = pmodel.epsilons[i] - 0.3\n",
    "    if lower_bound < lowest:\n",
    "        lower_bound = lowest\n",
    "    upper_bound = pmodel.epsilons[i] + 0.3\n",
    "    if upper_bound > highest:\n",
    "        upper_bound = highest\n",
    "    bounds.append([lower_bound,upper_bound])\n",
    "\n",
    "function_args = {\"bounds\":bounds, \"gtol\":0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, all the preliminaries are set up. We can run the whole thing now, and output the new Q value to see how much things have improved. When running the solver, you get a `EstimatorsObject` object as an output, here it's called `solutions`. Many values exist internally such as the new and old epsilons, the new and old Q values and some useful metrics such as how many times functions were called and how long each step took. \n",
    "\n",
    "For our particular problem, we will be using the L-BFGS-B algorithm as implemented in the scipy package. That is selected with the \"solver\" option. Now, all the variables we generated in Part II and part III are used to get the solutions.\n",
    "\n",
    "The solutions has several methods and attributes of note:\n",
    "\n",
    "**`EstimatorsObject.save_debug_files()`** : This saves several files containing the number of times each function is called. For example, if Q was computed 10 times and there were 170 discrete states, then there would be 170\\*10 = 170 calls to the Q function, once for each discrete state for each iteration of the parameter search. Also, a trace of the Q and -log(Q) values are also saved. \n",
    "\n",
    "**`EstimatorsObject.log_Qfunction_epsilon()`** : Takes epsilons as an input, and outputs the -log(Q) value.\n",
    "\n",
    "**`EstimatorsObject.Qfunction_epsilon()`** : Take epsilons as an input, and outputs the Q value. \n",
    "\n",
    "**`EstimatorsObject.old_epsilons`** : The starting set of epsilons. \n",
    "\n",
    "**`EstimatorsObject.new_epsilons`** : The new set of epsilons. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions = pyODEM.estimators.max_likelihood_estimate(data_formatted, obs, pmodel, stationary_distributions=stationary_distribution, solver=\"bfgs\", logq=True, kwargs=function_args)\n",
    "\n",
    "#optimization complete, results saved to the EstimatorsObject named solutions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the results. This is purely for your benefit to see how the Q values and epsilons changed.\n",
    "\n",
    "new_eps = solutions.new_epsilons # new epsilon values\n",
    "old_eps = solutions.old_epsilons # starting epsilon values\n",
    "Qold = solutions.oldQ # starting Q value\n",
    "Qnew = solutions.newQ # final Q value after optimization\n",
    "log_Qfunction = solutions.log_Qfunction_epsilon # function for computing the -log(Q)\n",
    "Q_function = solutions.Qfunction_epsilon # function for computing Q\n",
    "\n",
    "#Check and ensure that the starting and final Q values stored equal\n",
    "print Q_function(new_eps) == Qnew\n",
    "print Q_function(old_eps) == Qold\n",
    "\n",
    "# Print the starting and final Q values\n",
    "print\n",
    "print \"Q Functions:\"\n",
    "print \"Qold: %g\" %Qold\n",
    "print \"Qnew: %g\" %Qnew\n",
    "\n",
    "# Print the starting and final -log(Q) values\n",
    "print\n",
    "print \"Log Q Functions:\"\n",
    "print \"Qold: %g\" %log_Qfunction(old_eps)\n",
    "print \"Qnew: %g\" %log_Qfunction(new_eps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the optimization procedure complete, it is now time to save the new parameters and overwrite the ini_file if you wish to continue. This section makes heavy use of the `model_builder` package to write out the pairwise parameters. Recall, the `pmodel` (`Protein` object) has the attribute `pmodel.model` that contains the FiP35 backbone and pairwise parameters from Part I. In the end, you should have a new directory called \"newton_0\" (for iteration_0) that contains several files.\n",
    "\n",
    "Essential files:\n",
    "\n",
    "**pairwise_params** : A copy of the original pairwise_params_repulsive file. \n",
    "\n",
    "**model_params** : The new model_params file, analagous to the file in part I.\n",
    "\n",
    "\n",
    "Non-Essential files:\n",
    "\n",
    "**info.txt** : A info file written soley to keep track of changes to the Q value and temperatures used for the optimization\n",
    "\n",
    "**function_calls.dat** : A file containing the number of times each function was called during the search for the optimal parameters.\n",
    "\n",
    "**trace_log_Q_values.dat** : Each -log(Q) value computed during the search.\n",
    "\n",
    "**trace_Q_values.dat** : Each Q value computed during the search.\n",
    "\n",
    "Finally, the ww_domain.ini file is re-written if you wish to continue simulating and iterating. In the code box below, change `overwrite_ini_file = False` to `overwrite_ini_file = True` in order to overwrite the .ini file. In doing so, you will change the options for iteration 0 to the new values:\n",
    "\n",
    "\n",
    "*pairwise_params_file = newton_0/pairwise_params*\n",
    "\n",
    "*model_params = newton_0/model_params*\n",
    "\n",
    "*iteration = 1*\n",
    "\n",
    "Where if you were on iteration 1, you would get newton_1 and iteration = 2 instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite_ini_file = False\n",
    "\n",
    "# make a new directory to save all the debug files and the resultant parameters\n",
    "save_dir = \"%s/newton_%d\" % (cwd, iteration) \n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "    \n",
    "# the pmodel.save_model_parameters() method formats the epsilons to be saved.\n",
    "pmodel.save_model_parameters(new_eps) # load the new epsilons into the model object\n",
    "os.chdir(save_dir)\n",
    "solutions.save_debug_files() #optional if you wanted to get some statistics on number of certain function calls.\n",
    "\n",
    "# use model_builder to write out the pairwise parameter files\n",
    "writer = mdb.models.output.InternalFiles(pmodel.model) \n",
    "writer.write_pairwise_parameters() \n",
    "\n",
    "# write out an info file to keep track of what happened\n",
    "f = open(\"info.txt\", \"w\")\n",
    "f.write(\"computed at temperature: %d\\n\" % temperature)\n",
    "f.write(\"Qold: %g\\n\" % log_Qfunction(old_eps))\n",
    "f.write(\"Qnew: %g\\n\" % log_Qfunction(new_eps))\n",
    "f.close()\n",
    "os.chdir(cwd)\n",
    "\n",
    "# overwrite the ini file if overwrite_ini_file is True\n",
    "if overwrite_ini_file:\n",
    "    save_ini_file(model_name, iteration+1, \"newton_%d\" % iteration) #write a new ini file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusions\n",
    "These are all the steps for running a the pyODEM framework. Several things are ommitted for other tutorials, for example the model_builder page has examples for building the starting set of input files if required. The pyEMMA package has quite a few examples for detailing the analysis better. Reading those packages documentation and going through their examples would greatly improve your understanding for the steps shown in Part I and part II, and can greatly augment your understanding of the pyODEM framework. \n",
    "\n",
    "In any case, following these steps will be sufficient to get started with running and modifying simulations. Please, for any questions, please contact the students in Cecilia Clementi's group at Rice University."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Appendix\n",
    "A variety of methods are collected and demonstrated below. These were not necessary to know for running the pyODEM framework, but they could be of interest to the more advanced user as more customization becomes desired. Aside from the import statements, every section will be self-contained so that the user can use this independently of the rest of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_builder zero-indexing or one-indexing\n",
    "It was mentioned throughout the tutirla that model_builder sometimes uses zero-indexing or one-indexing. There is a definitive criteria our group has been using for the `model_builder` package: zero-index for things internally to the `model_builder` package, and one-index for things external to the `model_builder` package. This is relatively in line with the way other packages have handled this disconnect between the world and python, i.e. `mdtraj`. A few examples:\n",
    "1. Topology files for Gromacs, like .gro and .top files, are always indexed assuming starting with 1. When using these files by model_builder, it assumes this schematic was followed when reading those types of files.\n",
    "2. The atom pairs involved in a pairwise potential are always written and read as indexing starting from 1. However inside the code, this is automatically translated so using any methods inside, you would index from 0. This minimizes arithmetic when trying to determine residues for the atoms in the pairwise_potentials files when comparing to a .pdb file for example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise Potential Functions\n",
    "The pairwwise potential functions, as mentioned earlier in the notebook, come in many flavors. In particular, the FiP35 used two non-LeonnardJones like interactiosn that are probably worth looking at in greater detail. For this section, the code would be located in `model_builder/models/potentials/` for structure based and all-atom like models. First, load a model. If it is still iteration=0, then we should expect the nonnative pairwise parameters have epsilon=0 while the native pairwise potentials have epsilon=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model using model_builder. Recall could also load with pyODEM Protein object and access with Protein.model.\n",
    "model, fitopts = mdb.inputs.load_model(\"ww_domain.ini\") # model_builder reads the ww_domain.ini file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model` object has a attribute `Hamiltonian` which contains all the potentials in the model. Printing it, like so, prints out the number of each type of potential it finds. You can look inside the code `model_builder/models/potentials/hamiltonian.py` for how to access the other lists (i.e. `Hamiltonian._bonds` or `Hamiltonian._angles`, or `Hamiltonian._dihedrals`). For the pairwise potentials, we just have to use `Hamiltonian._pairs` and you will have a list of all the pairwise potentials. Each pairwise potential is an object of the class `PairPotential`, which can be seen inside the `model_builder/models/pairwise.py`, along with all the pairwise potentials that inherit from this base class. As a simple demonstrating, the basic information inside the pairwise potential can be printed out in the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# see model_builder/models/potentials/hamiltonian.py for generic Hamiltonian\n",
    "# see model_builder/models/potentials/sbmhamiltonian.py for methods specific to the structure-based hamiltonian\n",
    "# see model_builder/models/potentials/pairwise.py for all the pairwise potentials implemented\n",
    "\n",
    "print model.Hamiltonian \n",
    "print\n",
    "\n",
    "# access list of pairswise potentials:\n",
    "pairwise_potentials = model.Hamiltonian._pairs\n",
    "\n",
    "# print the first 10 pairwise_potential types and their epsilon and pairs involved\n",
    "# Reminder, the atom pairs are indexed starting from 0.\n",
    "print \"These are the first 10 pairwise potentials:\"\n",
    "for i in range(10):\n",
    "    atmi = pairwise_potentials[i].atmi.index # these atoms are mdtraj.atom objects, and contain a wealth of information\n",
    "    atmj = pairwise_potentials[i].atmj.index\n",
    "    name = pairwise_potentials[i].prefix_label\n",
    "    epsilon = pairwise_potentials[i].eps\n",
    "    print \"Pair between atoms: %d %d have a potential of type: %s and strength: %f\" % (atmi, atmj, name, epsilon)\n",
    "\n",
    "print\n",
    "print \"These are the LJ12Gaussian potentials in the first 200 pairwise potentials:\"\n",
    "# print the first 200 pairwise_potential ONLY IF they are only attractive LJ12 Gaussian Type potentials\n",
    "for i in range(200):\n",
    "    atmi = pairwise_potentials[i].atmi.index # these atoms are mdtraj.atom objects, and contain a wealth of information\n",
    "    atmj = pairwise_potentials[i].atmj.index\n",
    "    name = pairwise_potentials[i].prefix_label\n",
    "    epsilon = pairwise_potentials[i].eps\n",
    "    if name == \"LJ12GAUSSIAN\":\n",
    "        print \"Pair between atoms: %d %d have a potential of type: %s and strength: %f\" % (atmi, atmj, name, epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block below, we can start playing with the potentials. For example, we can use the first pairwise potential, a switchatble gaussian or tanh (attractive or repulsive) potential. By changing the values we can see how the different potentials look, and what the effect of changing epsilons are. In the very simple case, we plot it for epsilon = -1, 0, and 1 in the red, black, and blue curves respectively. Here, it is clear that the Tanh repulsive function isn't just a simple repulsive function, but rather adds a gradual step to prevent contact formation, but leaves the excluded volume mostly intact. \n",
    "\n",
    "Note: the same methods don't work for regular LJ12Gaussian potential. In general, all the pair-potentials were designed to be static, as in you can't change self.eps and expect everything to work. The special method `PairPotential.set_epsilon()` only exists for specific cases we wanted to allow it. If you wanted to plot the LJ12Gaussian potential at a variety of epsilons, you would have to initialize a new object for each epsilon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one of the potentials with zero epsilon and what does it look like.\n",
    "plot_pot = pairwise_potentials[0]\n",
    "print \"excluded_volume: %f   r0: %f   width: %f   epsilon: %f\" % (plot_pot.rNC, plot_pot.r0, plot_pot.width, plot_pot.eps)\n",
    "\n",
    "# a more itneresting demonstration is what does the potential look like with different epsilon values:\n",
    "\n",
    "r_possible = np.arange(0.1, 1.0, 0.01)\n",
    "\n",
    "# set a zero epsilon interaction\n",
    "plot_pot.set_epsilon(0)\n",
    "potential_zero = plot_pot.V(r_possible)\n",
    "\n",
    "# set a repulsive interaction\n",
    "plot_pot.set_epsilon(-1.) \n",
    "potential_negative = plot_pot.V(r_possible)\n",
    "\n",
    "# set an attractive interaction\n",
    "plot_pot.set_epsilon(1.)\n",
    "potential_positive = plot_pot.V(r_possible)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(r_possible, potential_zero, color=\"k\") # black curve\n",
    "plt.plot(r_possible, potential_negative, color=\"r\") # red curve\n",
    "plt.plot(r_possible, potential_positive, color=\"b\") # blue curve\n",
    "\n",
    "plt.axis([0, 1, -2, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
